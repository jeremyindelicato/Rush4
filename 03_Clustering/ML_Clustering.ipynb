{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Clustering - Segmentation Clients\n",
    "## Identification de Groupes Homog√®nes de Clients\n",
    "\n",
    "**Objectif :** Segmenter la base clients en groupes homog√®nes pour adapter les strat√©gies marketing\n",
    "\n",
    "**M√©thodes :**\n",
    "1. **K-Means** (partition bas√©e sur les centro√Ødes)\n",
    "2. **Hierarchical Clustering** (dendrogramme)\n",
    "3. **DBSCAN** (d√©tection d'outliers)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Phase 1 : Imports et Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "df = pd.read_csv('../01_Data/ML_DataSet.csv')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHARGEMENT DES DONN√âES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Shape : {df.shape}\")\n",
    "print(f\"\\nAper√ßu :\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Phase 2 : S√©lection des Features pour Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner les features pertinentes pour le clustering\n",
    "# On veut segmenter selon le COMPORTEMENT CLIENT\n",
    "\n",
    "features_clustering = [\n",
    "    # D√©mographiques\n",
    "    'Revenu',\n",
    "    'Age_Inscription',\n",
    "    'Total_Enfants',\n",
    "    \n",
    "    # Comportement d'achat\n",
    "    'Total_Depense',\n",
    "    'Total_Achats',\n",
    "    'Depense_Moy_Par_Achat',\n",
    "    \n",
    "    # Pr√©f√©rences produits\n",
    "    'Achat_Vins',\n",
    "    'Achat_Viandes',\n",
    "    'Achat_Poissons',\n",
    "    'Achat_Produits_Or',\n",
    "    \n",
    "    # Canaux d'achat\n",
    "    'Achats_En_Ligne',\n",
    "    'Achats_Catalogue',\n",
    "    'Achats_En_Magasin',\n",
    "    \n",
    "    # Engagement\n",
    "    'Visites_Web_Mois',\n",
    "    'Engagement_Web',\n",
    "    'Sensibilite_Promo',\n",
    "    \n",
    "    # Historique campagnes\n",
    "    'Total_Campagnes_Acceptees',\n",
    "    'Taux_Reponse_Historique'\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"S√âLECTION DES FEATURES POUR CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nNombre de features : {len(features_clustering)}\")\n",
    "print(\"\\nFeatures s√©lectionn√©es :\")\n",
    "for i, feat in enumerate(features_clustering, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Cr√©er le dataset pour clustering\n",
    "X_cluster = df[features_clustering].copy()\n",
    "\n",
    "# G√©rer les valeurs manquantes\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_cluster = pd.DataFrame(\n",
    "    imputer.fit_transform(X_cluster),\n",
    "    columns=features_clustering,\n",
    "    index=X_cluster.index\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset pr√©par√© : {X_cluster.shape}\")\n",
    "print(f\"‚úÖ Valeurs manquantes : {X_cluster.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives des features\n",
    "print(\"\\nüìä STATISTIQUES DES FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "X_cluster.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Phase 3 : Normalisation (OBLIGATOIRE pour Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation avec StandardScaler\n",
    "# CRUCIAL : les algorithmes de clustering sont sensibles √† l'√©chelle !\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "# Convertir en DataFrame pour garder les noms de colonnes\n",
    "X_scaled_df = pd.DataFrame(\n",
    "    X_scaled,\n",
    "    columns=features_clustering,\n",
    "    index=X_cluster.index\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NORMALISATION DES DONN√âES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAvant normalisation :\")\n",
    "print(f\"  Revenu : min={X_cluster['Revenu'].min():.0f}, max={X_cluster['Revenu'].max():.0f}\")\n",
    "print(f\"  Total_Depense : min={X_cluster['Total_Depense'].min():.0f}, max={X_cluster['Total_Depense'].max():.0f}\")\n",
    "\n",
    "print(f\"\\nApr√®s normalisation :\")\n",
    "print(f\"  Revenu : min={X_scaled_df['Revenu'].min():.2f}, max={X_scaled_df['Revenu'].max():.2f}\")\n",
    "print(f\"  Total_Depense : min={X_scaled_df['Total_Depense'].min():.2f}, max={X_scaled_df['Total_Depense'].max():.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es normalis√©es (moyenne=0, √©cart-type=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Phase 4 : K-Means Clustering\n",
    "\n",
    "### √âtape 1 : M√©thode du Coude (Elbow Method) pour trouver K optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester diff√©rentes valeurs de K\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RECHERCHE DU NOMBRE OPTIMAL DE CLUSTERS (K)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚è≥ Test de K=2 √† K=10...\\n\")\n",
    "\n",
    "for k in K_range:\n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # M√©triques\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_scaled, labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_scaled, labels)\n",
    "    \n",
    "    inertias.append(inertia)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    davies_bouldin_scores.append(davies_bouldin)\n",
    "    calinski_harabasz_scores.append(calinski_harabasz)\n",
    "    \n",
    "    print(f\"K={k:2d} | Inertia={inertia:8.0f} | Silhouette={silhouette:.3f} | DB={davies_bouldin:.3f} | CH={calinski_harabasz:.0f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tests termin√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des m√©triques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Elbow Method (Inertia)\n",
    "axes[0, 0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Nombre de clusters (K)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n",
    "axes[0, 0].set_title('Elbow Method - Recherche du K optimal', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(K_range)\n",
    "\n",
    "# 2. Silhouette Score (plus √©lev√© = meilleur)\n",
    "axes[0, 1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Nombre de clusters (K)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[0, 1].set_title('Silhouette Score (‚Üë meilleur)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(K_range)\n",
    "best_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "axes[0, 1].axvline(best_k_silhouette, color='red', linestyle='--', label=f'Meilleur K={best_k_silhouette}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Davies-Bouldin Score (plus bas = meilleur)\n",
    "axes[1, 0].plot(K_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Nombre de clusters (K)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Score', fontsize=12)\n",
    "axes[1, 0].set_title('Davies-Bouldin Score (‚Üì meilleur)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(K_range)\n",
    "best_k_db = K_range[np.argmin(davies_bouldin_scores)]\n",
    "axes[1, 0].axvline(best_k_db, color='red', linestyle='--', label=f'Meilleur K={best_k_db}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Calinski-Harabasz Score (plus √©lev√© = meilleur)\n",
    "axes[1, 1].plot(K_range, calinski_harabasz_scores, 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Nombre de clusters (K)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Score', fontsize=12)\n",
    "axes[1, 1].set_title('Calinski-Harabasz Score (‚Üë meilleur)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(K_range)\n",
    "best_k_ch = K_range[np.argmax(calinski_harabasz_scores)]\n",
    "axes[1, 1].axvline(best_k_ch, color='red', linestyle='--', label=f'Meilleur K={best_k_ch}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMANDATIONS POUR K OPTIMAL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Silhouette Score      ‚Üí K={best_k_silhouette}\")\n",
    "print(f\"  Davies-Bouldin Score  ‚Üí K={best_k_db}\")\n",
    "print(f\"  Calinski-Harabasz     ‚Üí K={best_k_ch}\")\n",
    "print(f\"\\nüí° K recommand√© : 3 ou 4 (bas√© sur le 'coude' et les m√©triques)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 2 : Clustering Final avec K optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir K optimal (ajuster selon les graphiques ci-dessus)\n",
    "K_OPTIMAL = 4  # √Ä ajuster selon vos r√©sultats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"K-MEANS CLUSTERING FINAL (K={K_OPTIMAL})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# K-Means final\n",
    "kmeans_final = KMeans(n_clusters=K_OPTIMAL, random_state=42, n_init=20)\n",
    "clusters_kmeans = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['Cluster_KMeans'] = clusters_kmeans\n",
    "\n",
    "# M√©triques finales\n",
    "silhouette_final = silhouette_score(X_scaled, clusters_kmeans)\n",
    "davies_bouldin_final = davies_bouldin_score(X_scaled, clusters_kmeans)\n",
    "calinski_harabasz_final = calinski_harabasz_score(X_scaled, clusters_kmeans)\n",
    "\n",
    "print(f\"\\nüìä M√©triques du clustering final :\")\n",
    "print(f\"   Silhouette Score      : {silhouette_final:.3f}\")\n",
    "print(f\"   Davies-Bouldin Score  : {davies_bouldin_final:.3f}\")\n",
    "print(f\"   Calinski-Harabasz     : {calinski_harabasz_final:.0f}\")\n",
    "\n",
    "print(f\"\\nüìã Distribution des clusters :\")\n",
    "print(df['Cluster_KMeans'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n‚úÖ Clustering K-Means termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Phase 5 : Analyse des Clusters\n",
    "\n",
    "### Profil de chaque segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un profil d√©taill√© de chaque cluster\n",
    "print(\"=\" * 70)\n",
    "print(\"PROFIL DES CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for cluster_id in range(K_OPTIMAL):\n",
    "    cluster_data = df[df['Cluster_KMeans'] == cluster_id]\n",
    "    n_clients = len(cluster_data)\n",
    "    pct_clients = (n_clients / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéØ CLUSTER {cluster_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nüìà Taille : {n_clients} clients ({pct_clients:.1f}% de la base)\")\n",
    "    \n",
    "    print(f\"\\nüí∞ PROFIL FINANCIER :\")\n",
    "    print(f\"   Revenu moyen             : {cluster_data['Revenu'].mean():>10,.0f} ‚Ç¨\")\n",
    "    print(f\"   D√©pense totale moyenne   : {cluster_data['Total_Depense'].mean():>10,.0f} ‚Ç¨\")\n",
    "    print(f\"   D√©pense moy. par achat   : {cluster_data['Depense_Moy_Par_Achat'].mean():>10,.2f} ‚Ç¨\")\n",
    "    print(f\"   Nombre d'achats moyen    : {cluster_data['Total_Achats'].mean():>10,.1f}\")\n",
    "    \n",
    "    print(f\"\\nüë§ PROFIL D√âMOGRAPHIQUE :\")\n",
    "    print(f\"   √Çge moyen                : {cluster_data['Age_Inscription'].mean():>10,.1f} ans\")\n",
    "    print(f\"   Enfants moyens           : {cluster_data['Total_Enfants'].mean():>10,.2f}\")\n",
    "    \n",
    "    print(f\"\\nüõí PR√âF√âRENCES PRODUITS :\")\n",
    "    print(f\"   Achat Vins               : {cluster_data['Achat_Vins'].mean():>10,.0f} ‚Ç¨\")\n",
    "    print(f\"   Achat Viandes            : {cluster_data['Achat_Viandes'].mean():>10,.0f} ‚Ç¨\")\n",
    "    print(f\"   Achat Poissons           : {cluster_data['Achat_Poissons'].mean():>10,.0f} ‚Ç¨\")\n",
    "    print(f\"   Achat Produits Or        : {cluster_data['Achat_Produits_Or'].mean():>10,.0f} ‚Ç¨\")\n",
    "    \n",
    "    print(f\"\\nüåê COMPORTEMENT DIGITAL :\")\n",
    "    print(f\"   Achats en ligne          : {cluster_data['Achats_En_Ligne'].mean():>10,.1f}\")\n",
    "    print(f\"   Achats catalogue         : {cluster_data['Achats_Catalogue'].mean():>10,.1f}\")\n",
    "    print(f\"   Achats en magasin        : {cluster_data['Achats_En_Magasin'].mean():>10,.1f}\")\n",
    "    print(f\"   Visites web/mois         : {cluster_data['Visites_Web_Mois'].mean():>10,.1f}\")\n",
    "    print(f\"   Engagement web           : {cluster_data['Engagement_Web'].mean():>10,.2%}\")\n",
    "    \n",
    "    print(f\"\\nüì¢ R√âACTIVIT√â MARKETING :\")\n",
    "    print(f\"   Taux de r√©ponse          : {cluster_data['Reponse_Derniere_Campagne'].mean():>10,.2%}\")\n",
    "    print(f\"   Campagnes accept√©es      : {cluster_data['Total_Campagnes_Acceptees'].mean():>10,.2f}\")\n",
    "    print(f\"   Sensibilit√© promo        : {cluster_data['Sensibilite_Promo'].mean():>10,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Analyse des clusters termin√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif des clusters\n",
    "features_compare = [\n",
    "    'Revenu', 'Total_Depense', 'Age_Inscription',\n",
    "    'Total_Achats', 'Achat_Vins', 'Achat_Viandes',\n",
    "    'Engagement_Web', 'Reponse_Derniere_Campagne'\n",
    "]\n",
    "\n",
    "cluster_profiles = df.groupby('Cluster_KMeans')[features_compare].mean()\n",
    "\n",
    "print(\"\\nüìä TABLEAU COMPARATIF DES CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "cluster_profiles.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations des Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA pour visualiser en 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Variance expliqu√©e\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PCA - R√âDUCTION DE DIMENSION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nVariance expliqu√©e par PC1 : {var_explained[0]:.2%}\")\n",
    "print(f\"Variance expliqu√©e par PC2 : {var_explained[1]:.2%}\")\n",
    "print(f\"Variance totale expliqu√©e  : {var_explained.sum():.2%}\")\n",
    "print(\"\\n‚úÖ PCA calcul√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation PCA 2D avec clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot 1 : Clusters K-Means\n",
    "scatter = axes[0].scatter(\n",
    "    X_pca[:, 0], \n",
    "    X_pca[:, 1], \n",
    "    c=clusters_kmeans, \n",
    "    cmap='viridis', \n",
    "    s=50, \n",
    "    alpha=0.6,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Ajouter les centro√Ødes\n",
    "centroids_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "axes[0].scatter(\n",
    "    centroids_pca[:, 0],\n",
    "    centroids_pca[:, 1],\n",
    "    c='red',\n",
    "    s=300,\n",
    "    marker='X',\n",
    "    edgecolors='black',\n",
    "    linewidth=2,\n",
    "    label='Centro√Ødes'\n",
    ")\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({var_explained[0]:.1%} variance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'PC2 ({var_explained[1]:.1%} variance)', fontsize=12)\n",
    "axes[0].set_title(f'Visualisation des {K_OPTIMAL} Clusters (K-Means + PCA)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2 : Distribution de la cible par cluster\n",
    "response_by_cluster = df.groupby('Cluster_KMeans')['Reponse_Derniere_Campagne'].mean()\n",
    "colors_bar = plt.cm.viridis(np.linspace(0, 1, K_OPTIMAL))\n",
    "axes[1].bar(range(K_OPTIMAL), response_by_cluster.values, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('Cluster', fontsize=12)\n",
    "axes[1].set_ylabel('Taux de R√©ponse', fontsize=12)\n",
    "axes[1].set_title('Taux de R√©ponse √† la Campagne par Cluster', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(K_OPTIMAL))\n",
    "axes[1].set_xticklabels([f'Cluster {i}' for i in range(K_OPTIMAL)])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, v in enumerate(response_by_cluster.values):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.1%}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualisations cr√©√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap des profils de clusters\n",
    "features_heatmap = [\n",
    "    'Revenu', 'Total_Depense', 'Age_Inscription', 'Total_Achats',\n",
    "    'Achat_Vins', 'Achat_Viandes', 'Achats_En_Ligne', 'Achats_En_Magasin',\n",
    "    'Visites_Web_Mois', 'Engagement_Web', 'Total_Campagnes_Acceptees',\n",
    "    'Reponse_Derniere_Campagne'\n",
    "]\n",
    "\n",
    "# Calculer les moyennes normalis√©es par cluster\n",
    "cluster_means = df.groupby('Cluster_KMeans')[features_heatmap].mean()\n",
    "\n",
    "# Normaliser pour la heatmap (entre 0 et 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_viz = MinMaxScaler()\n",
    "cluster_means_norm = pd.DataFrame(\n",
    "    scaler_viz.fit_transform(cluster_means.T).T,\n",
    "    columns=cluster_means.columns,\n",
    "    index=cluster_means.index\n",
    ")\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(\n",
    "    cluster_means_norm.T,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Valeur Normalis√©e (0-1)'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "plt.title('Heatmap des Profils de Clusters (Valeurs Normalis√©es)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation de la heatmap :\")\n",
    "print(\"   - Rouge fonc√© = valeur √©lev√©e\")\n",
    "print(\"   - Jaune clair = valeur faible\")\n",
    "print(\"   - Permet de comparer les clusters visuellement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üå≥ Phase 6 : Hierarchical Clustering (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering avec dendrogramme\n",
    "print(\"=\" * 70)\n",
    "print(\"HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚è≥ Calcul du dendrogramme (peut prendre 30s-1min)...\\n\")\n",
    "\n",
    "# Utiliser un √©chantillon pour le dendrogramme (trop lourd sinon)\n",
    "sample_size = min(500, len(X_scaled))\n",
    "sample_idx = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_idx]\n",
    "\n",
    "# Linkage\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "# Dendrogramme\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_font_size=10,\n",
    "    show_contracted=True\n",
    ")\n",
    "plt.title(f'Dendrogramme - Hierarchical Clustering (√©chantillon de {sample_size} clients)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Index des clients (ou clusters)', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.axhline(y=50, color='red', linestyle='--', label='Coupe sugg√©r√©e')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Dendrogramme cr√©√©\")\n",
    "print(\"üí° Observation : Le dendrogramme confirme la structure en 3-4 clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer Hierarchical Clustering sur toutes les donn√©es\n",
    "hierarchical = AgglomerativeClustering(n_clusters=K_OPTIMAL, linkage='ward')\n",
    "clusters_hierarchical = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "df['Cluster_Hierarchical'] = clusters_hierarchical\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HIERARCHICAL CLUSTERING - R√âSULTATS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDistribution des clusters :\")\n",
    "print(df['Cluster_Hierarchical'].value_counts().sort_index())\n",
    "\n",
    "# Comparer avec K-Means\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "ari = adjusted_rand_score(clusters_kmeans, clusters_hierarchical)\n",
    "print(f\"\\nüìä Similarit√© K-Means vs Hierarchical (ARI) : {ari:.3f}\")\n",
    "print(\"   (1.0 = identique, 0.0 = al√©atoire)\")\n",
    "\n",
    "print(\"\\n‚úÖ Hierarchical Clustering termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Phase 7 : DBSCAN (D√©tection d'Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN pour d√©tecter les outliers\n",
    "print(\"=\" * 70)\n",
    "print(\"DBSCAN - D√âTECTION D'OUTLIERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tester diff√©rents param√®tres\n",
    "eps_values = [1.5, 2.0, 2.5]\n",
    "min_samples = 10\n",
    "\n",
    "print(f\"\\n‚è≥ Test de diff√©rentes valeurs d'epsilon...\\n\")\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters_dbscan = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    n_clusters = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
    "    n_outliers = list(clusters_dbscan).count(-1)\n",
    "    \n",
    "    print(f\"eps={eps} | Clusters={n_clusters} | Outliers={n_outliers} ({n_outliers/len(clusters_dbscan)*100:.1f}%)\")\n",
    "\n",
    "# Appliquer DBSCAN avec epsilon optimal\n",
    "eps_optimal = 2.0\n",
    "dbscan_final = DBSCAN(eps=eps_optimal, min_samples=min_samples)\n",
    "clusters_dbscan = dbscan_final.fit_predict(X_scaled)\n",
    "\n",
    "df['Cluster_DBSCAN'] = clusters_dbscan\n",
    "\n",
    "n_outliers = list(clusters_dbscan).count(-1)\n",
    "print(f\"\\nüìä DBSCAN avec eps={eps_optimal} :\")\n",
    "print(f\"   Outliers d√©tect√©s : {n_outliers} ({n_outliers/len(df)*100:.1f}%)\")\n",
    "print(f\"\\n‚úÖ DBSCAN termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Phase 8 : Naming des Segments et Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donner des noms parlants aux clusters (√† adapter selon vos r√©sultats)\n",
    "# Bas√© sur l'analyse des profils ci-dessus\n",
    "\n",
    "cluster_names = {\n",
    "    0: \"Segment A - √Ä d√©finir\",\n",
    "    1: \"Segment B - √Ä d√©finir\",\n",
    "    2: \"Segment C - √Ä d√©finir\",\n",
    "    3: \"Segment D - √Ä d√©finir\"\n",
    "}\n",
    "\n",
    "# Exemples de noms possibles (√† adapter) :\n",
    "# - \"VIP - Gros D√©pensiers\"\n",
    "# - \"Digital Natives - Jeunes Connect√©s\"\n",
    "# - \"Occasionnels - Faible Engagement\"\n",
    "# - \"Seniors Fid√®les - Magasin\"\n",
    "\n",
    "df['Segment_Name'] = df['Cluster_KMeans'].map(cluster_names)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NAMING DES SEGMENTS\")\n",
    "print(\"=\" * 70)\n",
    "for cluster_id, name in cluster_names.items():\n",
    "    n_clients = len(df[df['Cluster_KMeans'] == cluster_id])\n",
    "    print(f\"\\nCluster {cluster_id} ‚Üí '{name}' ({n_clients} clients)\")\n",
    "\n",
    "print(\"\\nüí° Ajustez les noms dans la cellule ci-dessus selon vos analyses !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les profils des clusters\n",
    "features_export = [\n",
    "    'Revenu', 'Age_Inscription', 'Total_Enfants',\n",
    "    'Total_Depense', 'Total_Achats', 'Depense_Moy_Par_Achat',\n",
    "    'Achat_Vins', 'Achat_Viandes', 'Achat_Poissons',\n",
    "    'Achats_En_Ligne', 'Achats_En_Magasin', 'Visites_Web_Mois',\n",
    "    'Engagement_Web', 'Total_Campagnes_Acceptees',\n",
    "    'Reponse_Derniere_Campagne'\n",
    "]\n",
    "\n",
    "cluster_summary = df.groupby(['Cluster_KMeans', 'Segment_Name'])[features_export].agg([\n",
    "    'mean', 'median', 'std'\n",
    "]).round(2)\n",
    "\n",
    "# Sauvegarder\n",
    "cluster_summary.to_csv('cluster_profiles.csv')\n",
    "print(\"\\nüíæ Profils des clusters sauvegard√©s : cluster_profiles.csv\")\n",
    "\n",
    "# Sauvegarder le dataset avec les clusters\n",
    "df.to_csv('../01_Data/ML_DataSet_with_Clusters.csv', index=False)\n",
    "print(\"üíæ Dataset avec clusters sauvegard√© : ML_DataSet_with_Clusters.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Export termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Phase 9 : Conclusions et Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS DU CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚úÖ {K_OPTIMAL} segments de clients identifi√©s\")\n",
    "print(f\"\\nüìä M√©thodes utilis√©es :\")\n",
    "print(f\"   - K-Means (partition)\")\n",
    "print(f\"   - Hierarchical (dendrogramme)\")\n",
    "print(f\"   - DBSCAN (outliers)\")\n",
    "\n",
    "print(f\"\\nüéØ K-Means retenu comme m√©thode principale\")\n",
    "print(f\"   Silhouette Score : {silhouette_final:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS CL√âS :\")\n",
    "for cluster_id in range(K_OPTIMAL):\n",
    "    cluster_data = df[df['Cluster_KMeans'] == cluster_id]\n",
    "    name = cluster_names[cluster_id]\n",
    "    taux_reponse = cluster_data['Reponse_Derniere_Campagne'].mean()\n",
    "    depense_moy = cluster_data['Total_Depense'].mean()\n",
    "    print(f\"\\n   {name}\")\n",
    "    print(f\"      - Taux de r√©ponse : {taux_reponse:.1%}\")\n",
    "    print(f\"      - D√©pense moyenne : {depense_moy:.0f}‚Ç¨\")\n",
    "\n",
    "print(f\"\\nüéØ PROCHAINES √âTAPES :\")\n",
    "print(f\"   1. Affiner le naming des segments\")\n",
    "print(f\"   2. Cr√©er des strat√©gies marketing par segment\")\n",
    "print(f\"   3. Entra√Æner un mod√®le XGBoost par segment (optionnel)\")\n",
    "print(f\"   4. Monitorer l'√©volution des segments dans le temps\")\n",
    "\n",
    "print(f\"\\n‚úÖ Notebook de clustering termin√© avec succ√®s !\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
